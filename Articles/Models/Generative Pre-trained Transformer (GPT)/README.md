# Introduction

- OpenAI's [Generative Pre-trained Transformer (GPT)](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) is a group of autoregressive language models.
- GPT utilizes the decoder architecture of the standard Transformer network (with a few engineering adjustments) as an independent module. It is complemented by an extraordinary input size of 2048 tokens and 175 billion parameters, requiring approximately 800 GB of storage.
- GPT utilizes the decoder architecture from the standard Transformer network with some engineering modifications, functioning as a separate unit. It has an impressive input size of 2048 tokens and a remarkable 175 billion parameters, requiring about 800 GB of storage.
- It is trained to anticipate what the next token will be using the "generative pretraining" training approach. On several text-based tasks, the model demonstrated strong few-shot learning.
- The end result is the ability to generate human-like text with quick response time and high accuracy. Due to the extensive dataset and number of parameters (175B) in the GPT family of models, they often require minimal or no examples to fine-tune for downstream tasks using "prompt-based" fine-tuning. The text generated by GPT-3 is of such remarkable quality that it can be challenging to distinguish whether it was written by a human or the model, which presents both advantages and risks. [Source](https://analyticsindiamag.com/open-ai-gpt-3-language-model/)
- Before the advent of GPT,, language models (LMs) were usually trained on abundant but accurately labeled data, which was difficult to obtain. These LMs excelled in their specific supervised tasks but lacked ease in adapting to other tasks in different domains.
- On September 22, 2020, Microsoft announced that they had obtained an "exclusive" license to use GPT-3, granting them access to the underlying model, while others can still utilize the public API to receive output.
- In the following section, we will examine GPT1, GPT2, and GPT3 (with a particular emphasis on GPT-3, which is currently more prevalent) and their contributions to various Natural Language Processing tasks.


| | GPT-1| GPT-2| GPT-3| 
|--|:---:|:------:|:-----:|
|Parameters| 117 Million | 1.5 Billion | 175 Billion | 
| Decoder Layers | 12 | 48 | 96 |
| Context Token Size | 512 | 1024 | 2048 |
| Hidden Layer | 768 | 1600 | 12288|
| Batch Size | 64 | 512 | 3.2M|


# GPT-1: Improving Language Understanding by Generative Pre-Training






