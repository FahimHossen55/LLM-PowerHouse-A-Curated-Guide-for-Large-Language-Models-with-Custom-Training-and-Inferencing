# Overview 

- **Classification**
    - Accuracy: The proportion of predictions that are correct.
    - Precision: The proportion of positive predictions that are actually positive.
    - Recall: The proportion of actual positives that are correctly predicted.
    - F1 score: A harmonic mean of precision and recall.
    - Please refer the section on [Evaluation Metrics for the Classification Problem](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/tree/articles/Articles/Evaluation%20Metrics/Classification).

- **Generative Language Models**
    - [Perplexity](https://en.wikipedia.org/wiki/Perplexity): A measure of how well a language model predicts a sequence of words.
    - [Burstiness](https://machinelearning.wtf/terms/burstiness/#:~:text=If%20a%20term%20is%20used,significant%20than%20the%20first%20appearance): A measure of how likely a language model is to generate repetitive text.
- **Machine Translation/Captioning**
    -[ BLEU (BiLingual Evaluation Understudy)](https://en.wikipedia.org/wiki/BLEU): A measure of how similar a machine translation or caption is to human-generated translations or captions.
    - [CIDEr (CIDEr: Consensus-based Image Description Evaluation)](https://arxiv.org/abs/1411.5726): A measure of how similar a machine-generated image caption is to human-generated captions.
    - [METEOR (Metric for Evaluation of Translation with Explicit ORdering)](https://en.wikipedia.org/wiki/METEOR): A measure of how similar a machine translation is to a human-generated translation, taking into account word order.
- **Text Summarization**
    - [ROUGE (Recall-Oriented Understudy for Gisting Evaluation)](https://www.aclweb.org/anthology/W04-1013.pdf): A measure of how similar a machine-generated summary is to human-generated summaries.
- **Manual Evaluation by Humans**
    - [Mean Opinion Score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score): A measure of the overall quality of a system, typically obtained by asking human evaluators to rate the system on a scale of 1 to 5. MOS is used to evaluate a variety of NLP systems, including text generation systems, machine translation systems, text summarization systems, image generation systems, and recommendation systems.
- **NLP Benchmark Suites**
    - [GLUE (General Language Understanding Evaluation)](https://gluebenchmark.com/): A benchmark suite for evaluating the performance of NLP models on a variety of tasks, including natural language inference, sentiment analysis, and question answering.
    - [SuperGLUE (Super General Language Understanding Evaluation)](https://super.gluebenchmark.com/): A benchmark suite that is more challenging than GLUE, designed to evaluate the performance of NLP models on tasks that require reasoning and commonsense knowledge.

> Note: There is a wide variety of NLP evaluation metrics available, and the best metric to use will depend on the specific task and dataset. It is important to choose metrics that are appropriate for the task and that accurately reflect the performance of the model.


## Perplexity


